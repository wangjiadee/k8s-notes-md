

## 11、服务注册与服务发现

k8s的服务发现的总汇：

service环境变量和DNS两种模式：

- 环境变量：

当一个pod运行到了Node，kubelet会为每一个容器添加一组环境变量，Pod容器中的程序就可以使用这些变量发现service

环境变量名格式：

```
{SVCNAME}_SERVICE_HOST
{SVCNAME}_SERVICE_PORT
```

- DNS:

DNS服务监控kubernetes API，为每一个service创建DNS记录用于域名解析。这样Pod中就可以荣国DNS域名获取service的访问地址。

![image-20191216112746662](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216112746662.png)

service的发布服务：

1. clusterIP
2. NodePort
3. LoadBalancer





![image-20191216112724868](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216112724868.png

![image-20191213155439249](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191213155439249.png)



### 11.1service的三种类型

Service 是 Kubernetes 中最核心的概念，正是因为对此概念的支持，Kubernetes 在某种角度下可以被看成是一种微服务平台。Kubernetes 中的 pod 并不稳定，比如由ReplicaSet、Deployment、DaemonSet 等副本控制器创建的 pod，其副本数量、pod名称、pod 所运行的节点、pod 的 IP 地址等，会随着集群规模、节点状态、用户缩放等因素动态变化。Service 是一组逻辑 pod 的抽象，为一组 pod 提供统一入口，用户只需与service 打交道，service 提供 DNS 解析名称，负责追踪 pod 动态变化并更新转发表，通过负载均衡算法最终将流量转发到后端的 pod。

工作模式：userspace iptables，ipvs

​		~userspace： 1.1以前

​		~iptables： 1.10以前

​	     ~ipvs： 1.11之后

### 11.2 service代理模式--iptables工作原理

![image-20191215100933276](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191215100933276.png)

kube-proxy 的角色发生了变化。它在监控集群中的 service 与endpoint 时，不会在 local 网络上打开端口并设置 iptables 先将流量转发给自己，由自己分发给 pod。而是设置 iptable 将流量直接转发给 pod，转发给 pod 的工作由 kube-proxy转移到 iptables 中，也就是转移到内核空间。

iptables 模式安全，可靠、效率高，但因为受内核的限制不够灵活。



### 11.3service代理模式IPVS工作原理

iptables 模式与 ipvs 模式本质相同，实现细节不同。前者首先定义规则，表示规则的数据 保存在内核中，即通常说的“四表五链”。然后内核根据"四表五链"中的数据创建相应函数 并挂载到内核中合适的点上。在 ipvs 模式中，kube-proxy 根据其对 servcie、endpoin的监控结果，调用内核 netlink 接口创建 ipvs rule，不同于 iptables 的"四表五链"，ipvs rule的数据组织更加紧凑、高效。因此相对于 iptabels 模式，ipvs 模式更节省资源，对 servcie、 endpoint 的变更同步速度更快。另外 ipvs 支持更多种类的负载均衡算法：

```
rr: round-robin/轮询
lc: least connection/最少连接
dh: destination hashing/目标哈希
sh: source hashing/源哈希
sed: shortest expected delay/预计延迟时间最短
nq: never queue/从不排队
```

LVS 基于 IPVS内核调度模块实现的负载均衡。

![img](https://img2018.cnblogs.com/blog/1183448/201908/1183448-20190826174639595-375486900.png)

```
[root@k8s-master1 ~]# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
```



### 11.4Service 介 绍	[ClusterIP,NodePort,LoadBalancer]

#### 11.4.1 ClusterIP

Service 默认类型，分配一个集群内部可以访问的虚拟IP(VIP)，同一个集群内部应用之间相互访问

![image-20191215101613626](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191215101613626.png)

```yaml
apiVersion: v1 
kind: Service 
metadata:
  name: gomo-service 
spec:
  selector: 
    app: gomo    #即标签选择器会去查找label app为gomo的pod
  ports:
  - protocol: TCP 
    port: 80 
    targetPort: 8080
```

#### 11.4.2NodePort

在每个Node上分配一个端口作为外部访问入口, 让外部用户访问到集群内部pod应用

![image-20191215101933140](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191215101933140.png)

```yaml
apiVersion: v1 
kind: Service 
metadata:
  name: my-nodeport-service 
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30001   ##可指定也可以不指定，不指定会自动分配端口
  type: NodePort
```

```yaml
# kubectl create -f my-NodePort-service.yaml

# kubectl get svc
NAME                  TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
kubernetes            ClusterIP   10.0.0.1     <none>        443/TCP        5d2h
my-nodeport-service   NodePort    10.0.0.105   <none>        80:30001/TCP   4m39s

# kubectl get ep
NAME                  ENDPOINTS                                      AGE
kubernetes            10.40.6.201:6443,10.40.6.209:6443              5d2h
my-nodeport-service   172.17.31.3:80,172.17.59.2:80,172.17.59.3:80   5m32s

# netstat -ntlp |grep 30001   ##每个node节点上启动了一个kube-proxy进程，并监听30001端口
tcp6       0      0 :::30001                :::*                    LISTEN      28035/kube-proxy   
```

用浏览器打开nodeIP:30001即可访问到pod应用。在node节点上使用 `ipvsadm -ln`可以看到很多做端口轮询转发的规则。如果要提供给外用户访问，在前面再加个负载均衡器，比如nginx，haproxy,或公有云的LB，转发到指定的某几台node，域名解析到负载均衡器即可。

#### 11.4.3 LoadBalance

工作在特定的Cloud Provider上，例如Google Cloud，AWS，OpenStack, 不是我们自建的kubernetes集群里，是公有云提供商提供，公有云LB可以自动将我们node 上的service 的IP和端口加入LB中。

![image-20191215102222129](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191215102222129.png)

### 11.5Service在Kubernetes中工作原理

![image-20191215103610404](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191215103610404.png)

- 为什么Servcie能定位到Pod

  因为Pod的IP是不固定的，所以Kubernetes需要Service，除此之外它还可以在多个Pod间负载均衡Service的访问入口，其实是宿主机的kube-proxy生成的iptables规则 ，及kube-dns生成的DNS记录

  Service通过label标签选中Pod，被选中的的Pod称为Service的Endpoints

```
[root@k8s-master1 ~]# kubectl get ep
NAME         ENDPOINTS                                                        AGE
kubernetes   192.168.208.191:6443,192.168.208.192:6443,192.168.208.193:6443   2d17h
nginx-ds                                                                      2d14h

```

可以看到，当我们访问kubernetes这个service时，会被定位到192.168.208.191:6443,192.168.208.192:6443,192.168.208.193:6443 
注意：只有处于Running状态，且readlinessProbe检查通过的Pod，才会出现在Endpoints列表里service列表的VIP（ClusterIP），是设置了一个固定的入口地址，并没有真正的网络设备，ping是没有响应的
通过访问10.111.15.33，就可以访问到它所代理的Pod了
Service的实现原理是由kube-proxy组件，加上iptables来共同实现的，当提交service后，会创建一条iptables规则

### 11.6DNS

service CLUSTER-IP 也不是固定不变的，在应用程序也不可能写CLUSTER-IP，这里建议写域名(service名称)，kubernetes集群DNS 将service 名称解析为CLUSTER-IP，DNS服务实时监视Kubernetes API，为每一个Service创建DNS记录用于域名解析

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health
        ready
        kubernetes cluster.local. in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        beta.kubernetes.io/os: linux
      containers:
      - name: coredns
        image: coredns/coredns:1.6.2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: {{CLUSTER_DNS}}
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
```





### 11.7Ingress作原理与规则

![image-20191216135743766](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216135743766.png)

Ingress exposes HTTP and HTTPS routes from outside the cluster to [services](https://kubernetes.io/docs/concepts/services-networking/service/) within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.

```
    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
```

An Ingress can be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name based virtual hosting. An [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers) is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.

An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type [Service.Type=NodePort](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport) or [Service.Type=LoadBalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer).

- Prerequisites

You must have an [ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers) to satisfy an Ingress. Only creating an Ingress resource has no effect.

You may need to deploy an Ingress controller such as [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/). You can choose from a number of [Ingress controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers).

Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly differently.

> **Note:** Make sure you review your Ingress controller’s documentation to understand the caveats of choosing it.

- The Ingress Resource

A minimal Ingress resource example:

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          serviceName: test
          servicePort: 80
```

As with all other Kubernetes resources, an Ingress needs `apiVersion`, `kind`, and `metadata` fields. For general information about working with config files, see [deploying applications](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/), [configuring containers](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/), [managing resources](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/). Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the [rewrite-target annotation](https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md). Different [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers) support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.

The Ingress [spec](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP traffic.

- Ingress rules

Each HTTP rule contains the following information:

- An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host.
- A list of paths (for example, `/testpath`), each of which has an associated backend defined with a `serviceName` and `servicePort`. Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service.
- A backend is a combination of Service and port names as described in the [Service doc](https://kubernetes.io/docs/concepts/services-networking/service/). HTTP (and HTTPS) requests to the Ingress that matches the host and path of the rule are sent to the listed backend.

A default backend is often configured in an Ingress controller to service any requests that do not match a path in the spec.

- Default Backend

An Ingress with no rules sends all traffic to a single default backend. The default backend is typically a configuration option of the [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers) and is not specified in your Ingress resources.

If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.

- Types of Ingress
- Single Service Ingress

There are existing Kubernetes concepts that allow you to expose a single Service (see [alternatives](https://kubernetes.io/docs/concepts/services-networking/ingress/#alternatives)). You can also do this with an Ingress by specifying a *default backend* with no rules.

| [`service/networking/ingress.yaml`](https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/service/networking/ingress.yaml) ![Copy service/networking/ingress.yaml to clipboard](https://d33wubrfki0l68.cloudfront.net/951ae1fcc65e28202164b32c13fa7ae04fab4a0b/b77dc/images/copycode.svg) |
| ------------------------------------------------------------ |
| `apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata:  name: test-ingress spec:  backend:    serviceName: testsvc    servicePort: 80 ` |

If you create it using `kubectl apply -f` you should be able to view the state of the Ingress you just added:

```shell
kubectl get ingress test-ingress
NAME           HOSTS     ADDRESS           PORTS     AGE
test-ingress   *         107.178.254.228   80        59s
```

Where `107.178.254.228` is the IP allocated by the Ingress controller to satisfy this Ingress.

> **Note:** Ingress controllers and load balancers may take a minute or two to allocate an IP address. Until that time, you often see the address listed as ``.

- Simple fanout

A fanout configuration routes traffic from a single IP address to more than one Service, based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers down to a minimum. For example, a setup like:

```none
foo.bar.com -> 178.91.123.132 -> / foo    service1:4200
                                 / bar    service2:8080
```

would require an Ingress such as:

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: simple-fanout-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: service1
          servicePort: 4200
      - path: /bar
        backend:
          serviceName: service2
          servicePort: 8080
```

When you create the Ingress with `kubectl apply -f`:

```shell
kubectl describe ingress simple-fanout-example
Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
```

The Ingress controller provisions an implementation-specific load balancer that satisfies the Ingress, as long as the Services (`service1`, `service2`) exist. When it has done so, you can see the address of the load balancer at the Address field.

> **Note:** Depending on the [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers) you are using, you may need to create a default-http-backend [Service](https://kubernetes.io/docs/concepts/services-networking/service/).

- Name based virtual hosting

Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.

```none
foo.bar.com --|                 |-> foo.bar.com service1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-> bar.foo.com service2:80
```

The following Ingress tells the backing load balancer to route requests based on the [Host header](https://tools.ietf.org/html/rfc7230#section-5.4).

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
```

If you create an Ingress resource without any hosts defined in the rules, then any web traffic to the IP address of your Ingress controller can be matched without a name based virtual host being required.

For example, the following Ingress resource will route traffic requested for `first.bar.com` to `service1`, `second.foo.com` to `service2`, and any traffic to the IP address without a hostname defined in request (that is, without a request header being presented) to `service3`.

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: first.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: second.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
  - http:
      paths:
      - backend:
          serviceName: service3
          servicePort: 80
```

- TLS

You can secure an Ingress by specifying a [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) that contains a TLS private key and certificate. Currently the Ingress only supports a single TLS port, 443, and assumes TLS termination. If the TLS configuration section in an Ingress specifies different hosts, they are multiplexed on the same port according to the hostname specified through the SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret must contain keys named `tls.crt` and `tls.key` that contain the certificate and private key to use for TLS. For example:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
  namespace: default
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
type: kubernetes.io/tls
```

Referencing this secret in an Ingress tells the Ingress controller to secure the channel from the client to the load balancer using TLS. You need to make sure the TLS secret you created came from a certificate that contains a Common Name (CN), also known as a Fully Qualified Domain Name (FQDN) for `sslexample.foo.com`.

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: tls-example-ingress
spec:
  tls:
  - hosts:
    - sslexample.foo.com
    secretName: testsecret-tls
  rules:
    - host: sslexample.foo.com
      http:
        paths:
        - path: /
          backend:
            serviceName: service1
            servicePort: 80
```

> **Note:** There is a gap between TLS features supported by various Ingress controllers. Please refer to documentation on [nginx](https://git.k8s.io/ingress-nginx/README.md#https), [GCE](https://git.k8s.io/ingress-gce/README.md#frontend-https), or any other platform specific Ingress controller to understand how TLS works in your environment.

- Loadbalancing

An Ingress controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (e.g. persistent sessions, dynamic weights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service.

It’s also worth noting that even though health checks are not exposed directly through the Ingress, there exist parallel concepts in Kubernetes such as [readiness probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/) that allow you to achieve the same end result. Please review the controller specific documentation to see how they handle health checks ( [nginx](https://git.k8s.io/ingress-nginx/README.md), [GCE](https://git.k8s.io/ingress-gce/README.md#health-checks)).

- Updating an Ingress

To update an existing Ingress to add a new Host, you can update it by editing the resource:

```shell
kubectl describe ingress test
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
kubectl edit ingress test
```

This pops up an editor with the existing configuration in YAML format. Modify it to include the new Host:

```yaml
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
        path: /foo
  - host: bar.baz.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
        path: /foo
..
```

After you save your changes, kubectl updates the resource in the API server, which tells the Ingress controller to reconfigure the load balancer.

Verify this:

```shell
kubectl describe ingress test
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
```

You can achieve the same outcome by invoking `kubectl replace -f` on a modified Ingress YAML file.

- Failing across availability zones

Techniques for spreading traffic across failure domains differs between cloud providers. Please check the documentation of the relevant [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers) for details. You can also refer to the [federation documentation](https://github.com/kubernetes-sigs/federation-v2) for details on deploying Ingress in a federated cluster.

- Future Work

Track [SIG Network](https://github.com/kubernetes/community/tree/master/sig-network) for more details on the evolution of Ingress and related resources. You may also track the [Ingress repository](https://github.com/kubernetes/ingress/tree/master) for more details on the evolution of various Ingress controllers.

- Alternatives

You can expose a Service in multiple ways that don’t directly involve the Ingress resource:

- Use [Service.Type=LoadBalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer)
- Use [Service.Type=NodePort](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport)



11.8.1  lngress-nginx的安装：

The following **Mandatory Command** is required for all deployments.

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
```

Tip：

If you are using a Kubernetes version previous to 1.14, you need to change `kubernetes.io/os` to `beta.kubernetes.io/os` at line 217 of [mandatory.yaml](https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/mandatory.yaml#L217), see [Labels details](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/).

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      # wait up to five minutes for the drain of connections
      terminationGracePeriodSeconds: 300
      serviceAccountName: nginx-ingress-serviceaccount
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: nginx-ingress-controller
          image: 192.168.208.195/room/nginx-ingress-controller:0.26.1
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown

---

```



### 11.8Ingress Controllers工作原理与部署

![image-20191216100838081](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216100838081.png)

```
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      # wait up to five minutes for the drain of connections
      terminationGracePeriodSeconds: 300
      serviceAccountName: nginx-ingress-serviceaccount
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

```

```
[root@k8s-master1 ~]# kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-568867bf56-mxr6q   1/1     Running   0          24s
[root@k8s-master1 ~]# kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.254.35.24   <none>        80:30909/TCP,443:32097/TCP   41s
```

![image-20191216100955560](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216100955560.png)

![image-20191216101005894](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216101005894.png)

配置成功 400 和404 是我们 没有配置后端的东西

### 11.9 使用lngress发布http/https网站

1为ingress 位置后端网站

```yaml
apiVersion: v1
kind: Service
metadata:
  name: gomo-wbs
  namespace: default
spec:
  selector:
    app: gomo-wbs
  clusterIP: ""  #设置无头服务
  ports:
  - port: 80
    targetPort: 80 

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wbs-deploy
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      app: gomo-wbs
  template:
    metadata:
      labels:
        app: gomo-wbs
    spec:
      containers:
      - name: wbs
        image: wangjiadee/wbs:latest
        ports:
        - name: http
          containerPort: 80

```

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wbs
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: gomo.wbs.com
    http:
      paths:
      - path: /
        backend:
          serviceName: gomo-wbs
          servicePort: 80
   #通过Ingress把myapp-svc发布出去
   #namespace要和deployment和要发布的service处于同一个名称空间
   #annotations:说明我们要用到的ingress-controller是nginx,而不是Traefik、Envoy
   			#kubernetes.io/ingress.class: "nginx"
   #host:表示访问这个域名,就会转发到后端gomo-wbs管理的pod上
   
```



```
[root@k8s-master1 ~]# kubectl get ingress
NAME          HOSTS          ADDRESS        PORTS   AGE
ingress-wbs   gomo.wbs.com   10.254.35.24   80      161m
[root@k8s-master1 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
gomo-wbs     ClusterIP   10.254.235.115   <none>        80/TCP         84s
kubernetes   ClusterIP   10.254.0.1       <none>        443/TCP        18h
nginx-ds     NodePort    10.254.254.204   <none>        80:32138/TCP   18h
[root@k8s-master1 ~]# kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.254.35.24   <none>        80:30909/TCP,443:32097/TCP   3h38m

#在对应的worker 节点用域名访问

[root@k8s-worker3 ~]# curl gomo.wbs.com:30909

<!doctype html>
<html lang="zh-TW" class="no-js">
	<head>
		<meta charset="UTF-8">
		<title> 玩錶舍</title>

		<link href="//www.google-analytics.com" rel="dns-prefetch">
        <link href="https://secureservercdn.net/166.62.112.199/z9n.3b8.myftpupload.com/wp-content/themes/rx8game/img/icons/favicon.png" rel="shortcut icon">
        <link href="https://secureservercdn.net/166.62.112.199/z9n.3b8.myftpupload.com/wp-content/themes/rx8game/img/icons/touch.png" rel="apple-touch-icon-precomposed">

		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta name="description" content="玩錶社-頂級腕錶保護膜的專家，針對高級腕錶設計出服貼、高亮度貼膜， 不需要使用任何刀具，對於邊角、彎邊、導角，製造出最頂級的裁切，不受高溫、海蝕、長時間使用的限制！">

								玩錶舍<br>也歡迎您親自蒞臨！<br class="d-sm-none">台北市信義區忠孝東路五段264號<br>
								我們以客戶為尊，<br class="d-sm-none">歡迎您在營業時間內光臨指教。<br><br>
								營業時間<br>
								星期一到六 12:00-21:00
								<p>
.....lue.......

	</body>
</html>

```



2 设置HTTPs的网站：

自签证书

```
[root@k8s-master1 ~]# openssl genrsa -out tls.key 2048
Generating RSA private key, 2048 bit long modulus
.....................+++
.......................................................................................................................+++
e is 65537 (0x10001)
[root@k8s-master1 ~]# openssl req -new -x509 -key tls.key  -out tls.crt -subj /C=CN/ST=gomo/O=gomo/CN=gomo.wbs.com
[root@k8s-master1 ~]# kubectl create secret tls myapp-infress-secret --cert=tls.crt --key=tls.key
secret/myapp-infress-secret created
```

注入到yaml文件中：

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wbs-tls
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - gomo.wbs.com
    secretName: myapp-infress-secret
  rules:
  - host: gomo.wbs.com
    http:
      paths:
      - path: /
        backend:
          serviceName: gomo-wbs
          servicePort: 80
```

在worker上检测：

![image-20191216135621942](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216135621942.png)

### 11.10 使用annotations发布对lngress做制定话配置（重定向）

```
[root@k8s-master1 ~]# yum -y install httpd
[root@k8s-master1 ~]# htpasswd -c gomo gomo
New password: 
Re-type new password: 
Adding password for user gomo
# 账号密码 都是gomo
[root@k8s-master1 ~]# kubectl create secret generic basic-gomo --from-file=gomo
secret/basic-gomo created
[root@k8s-master1 ~]# kubectl describe secret basic-gomo
Name:         basic-gomo
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
gomo:  43 bytes

```

创建一个新的ingress：

```yaml
apiVersion: extensions/v1beta1
kind: Ingress 
metadata:
  name: ingress-with-auth 
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic 
    nginx.ingress.kubernetes.io/auth-secret:  basic-gomo 
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - gomo' 
spec:
  rules:
  - host: gomo.wbs.com 
    http:
      paths:
      - path: / 
        backend:
          serviceName: gomo-wbs
          servicePort: 80
   
---
apiVersion: extensions/v1beta1 
kind: Ingress 
metadata:
  name: gomo-wbs 
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: http://gomo.wbs.com:30909
spec:
  rules:
  - host: gomo.wbs.com 
    http:
      paths:
      - path: / 
        backend:
          serviceName: gomo-wbs 
          servicePort: 80
```



### 11.11 lngress controller高可用方案

..............

说白了 就是调整nginx lngress 的replica数量

![image-20191216141632083](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191216141632083.png)



```
  ~ # 1> 这里通过scale命令来扩容到3个Pod副本（可依据具体业务流量来设置为合适的值）
  ~ kubectl -n kube-system scale --replicas=3 deployment/nginx-ingress-controller
deployment.extensions "nginx-ingress-controller" scaled
  ~ 
  ~
  ~ # 2> 查看当前Pod副本情况
  ~ kubectl -n kube-system get pod | grep nginx-ingress-controller
nginx-ingress-controller-674c96ffbc-7h4nt                    1/1       Running   0          4h
nginx-ingress-controller-674c96ffbc-rvfcw                    1/1       Running   0          4h
nginx-ingress-controller-674c96ffbc-xm8dw                    1/1       Running   0    

 ~ # 1> 查看当前集群节点情况
  ~ kubectl get node
NAME                                 STATUS    ROLES     AGE       VERSION
k8s-worker3                          Ready     <none>    4h        v1.13.4
k8s-worker2                          Ready     <none>    4h        v1.13.4
k8s-worker1                          Ready     <none>    4h        v1.13.4
k8s-master3                          Ready     master    4h        v1.13.4
k8s-master2                          Ready     master    4h        v1.13.4
k8s-master1                          Ready     master    4h        v1.13.4
  ~
  ~
  ~ # 2> 若我们希望部署在k8s-worker2和k8s-worker1两个节点上，
  ~ #    我们可以先给这两个节点打上标 node-role.kubernetes.io/ingress="true"
  ~ kubectl label nodes k8s-worker1 node-role.kubernetes.io/ingress="true"
node "k8s-worker1" labeled
  ~ kubectl label nodes k8s-worker2 node-role.kubernetes.io/ingress="true"
node "k8s-worker2" labeled
  ~
  ~
  ~ # 3> 然后我们更新deployment增加nodeSelector配置
  ~ kubectl -n kube-system patch deployment nginx-ingress-controller -p '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/ingress": "true"}}}}}'
deployment.extensions "nginx-ingress-controller" patched



##@@@@@@@@@@@@@@@@@@@@@@@@##
1）确保打标节点数不少于Pod副本数以尽量规避多个Pod运行在同一个节点上
2）不建议部署到master节点上
```



### 11.12 Headless 服务

$$
spec:clusterIP: None
$$

```
redis-test                                 ClusterIP      None             <none>         42/TCP           277d

[root@k8s-master-1:~]# nslookup redis-test.default.svc.cluster.local 172.22.0.36
Server:		172.22.0.36
Address:	172.22.0.36#53
#172.22.0.36 为k8s的dns ip
Name:	redis-test.default.svc.cluster.local
Address: 172.22.3.119

```

因为没有ClusterIP，kube-proxy 并不处理此类服务，因为没有load balancing或 proxy 代理设置，在访问服务的时候回返回后端的全部的Pods IP地址，主要用于开发者自己根据pods进行负载均衡器的开发(设置了selector

