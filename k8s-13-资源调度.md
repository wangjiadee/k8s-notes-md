# 13、资源调度

当Scheduler通过API server 的watch接口监听到新建Pod副本的信息后，它会检查所有符合该Pod要求的Node列表，开始执行Pod调度逻辑。调度成功后将Pod绑定到目标节点上。Scheduler在整个系统中承担了承上启下的作用，承上是负责接收创建的新Pod，为安排一个落脚的地（Node）,启下是安置工作完成后，目标Node上的kubelet服务进程接管后继工作，负责Pod生命周期的后半生。具体来说，Scheduler的作用是将待调度的Pod安装特定的调度算法和调度策略绑定到集群中的某个合适的Node上，并将绑定信息传给API server 写入etcd中。整个调度过程中涉及三个对象，分别是：待调度的Pod列表，可以的Node列表，以及调度算法和策略。

## 13.1调度流程

Kubernetes Scheduler 提供的调度流程分三步：

1. 预选策略(predicate) 遍历nodelist，选择出符合要求的候选节点，Kubernetes内置了多种预选规则供用户选择。

2. 优选策略(priority) 在选择出符合要求的候选节点中，采用优选规则计算出每个节点的积分，最后选择得分最高的。

3. 选定(select) 如果最高得分有好几个节点，select就会从中随机选择一个节点。

   ![image-20200102164430188](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20200102164430188.png)

   预选策略算法的集合在[官方源码](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/algorithm/predicates/predicates.go)

## 13.2调度策略

Kubernetes中的调度策略主要分为**全局调度**与**运行时调度**2种。其中全局调度策略在调度器启动时配置，而运行时调度策略主要包括选择节点（nodeSelector），节点亲和性（nodeAffinity），pod亲和与反亲和性（podAffinity与podAntiAffinity）。Node Affinity、podAffinity/AntiAffinity以及后文即将介绍的污点(Taints）与容忍（tolerations）等特性。

### 13.2.1NodeSelector



```
#我们定义一个pod,让其选择带有work=gomo这个标签的节点
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    name: myapp
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  nodeSelector:
    worker: gomo

#创建
[root@adm-master1 home]# kubectl create -f NodeSelector.yaml -n gomo-test
pod/pod-1 created

#发现是pending状态
[root@adm-master1 home]# kubectl get all -n gomo-test
NAME        READY   STATUS    RESTARTS   AGE
pod/pod-1   0/1     Pending   0          9s
[root@adm-master1 home]# kubectl describe pods pod-1 -n gomo-test
Name:         pod-1
Namespace:    gomo-test
Priority:     0
Node:         <none>
Labels:       name=myapp
Annotations:  <none>
Status:       Pending
IP:           
IPs:          <none>
Containers:
  myapp:
    Image:        ikubernetes/myapp:v1
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cxfdb (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  default-token-cxfdb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-cxfdb
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  worker=gomo
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/7 nodes are available: 7 node(s) didn't match node selector.
```

```
#我们给随便给一个worker打上这个标签
[root@adm-master1 home]# kubectl label node adm-worker1 worker=gomo
node/adm-worker1 labeled


#查看标签：
[root@adm-master1 home]# kubectl get nodes --show-labels
NAME          STATUS   ROLES    AGE    VERSION   LABELS
.....omit.....
adm-worker1   Ready    <none>   2d5h   v1.17.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=adm-worker1,kubernetes.io/os=linux,worker=gomo


#然后再次查看 
[root@adm-master1 home]# kubectl describe pods pod-1 -n gomo-test
.....omit.....
Events:
  Type     Reason            Age        From                  Message
  ----     ------            ----       ----                  -------
  Warning  FailedScheduling  <unknown>  default-scheduler     0/7 nodes are available: 7 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler     0/7 nodes are available: 7 node(s) didn't match node selector.
  Normal   Scheduled         <unknown>  default-scheduler     Successfully assigned gomo-test/pod-1 to adm-worker1
  Normal   Pulling           22s        kubelet, adm-worker1  Pulling image "ikubernetes/myapp:v1"
  Normal   Pulled            17s        kubelet, adm-worker1  Successfully pulled image "ikubernetes/myapp:v1"
  Normal   Created           17s        kubelet, adm-worker1  Created container myapp
  Normal   Started           17s        kubelet, adm-worker1  Started container myapp
[root@adm-master1 home]# kubectl get all -n gomo-test
NAME        READY   STATUS    RESTARTS   AGE
pod/pod-1   1/1     Running   0          3m18s
```

### 13.2.2nodeAffinity(节点亲和性)

```
结构图：
pod
└── spec
    └── affinity
        └── nodeAffinity
            ├── preferredDuringSchedulingIgnoredDuringExecution
            │   ├── preference
            │   └── weight
            └── requiredDuringSchedulingIgnoredDuringExecution
                └── nodeSelectorTerms
                    ├── matchExpressions
                    └── matchFields

#############################################详细内容################################################


[root@adm-master1 home]# kubectl explain pod.spec.affinity.nodeAffinity 
KIND:     Pod
VERSION:  v1

RESOURCE: nodeAffinity <Object>

DESCRIPTION:
     Describes node affinity scheduling rules for the pod.

     Node affinity is a group of node affinity scheduling rules.

FIELDS:
   preferredDuringSchedulingIgnoredDuringExecution	<[]Object>
   （ 软亲和性 能满足最好，不满足也没关系）
     The scheduler will prefer to schedule pods to nodes that satisfy the
     affinity expressions specified by this field, but it may choose a node that
     violates one or more of the expressions. The node that is most preferred is
     the one with the greatest sum of weights, i.e. for each node that meets all
     of the scheduling requirements (resource request, requiredDuringScheduling
     affinity expressions, etc.), compute a sum by iterating through the
     elements of this field and adding "weight" to the sum if the node matches
     the corresponding matchExpressions; the node(s) with the highest sum are
     the most preferred.

   requiredDuringSchedulingIgnoredDuringExecution	<Object>
   （硬亲和性  必须满足亲和性）
     If the affinity requirements specified by this field are not met at
     scheduling time, the pod will not be scheduled onto the node. If the
     affinity requirements specified by this field cease to be met at some point
     during pod execution (e.g. due to an update), the system may or may not try
     to eventually evict the pod from its node.

[root@adm-master1 home]# kubectl explain pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecutionKIND:     Pod
VERSION:  v1

RESOURCE: preferredDuringSchedulingIgnoredDuringExecution <[]Object>

DESCRIPTION:
     The scheduler will prefer to schedule pods to nodes that satisfy the
     affinity expressions specified by this field, but it may choose a node that
     violates one or more of the expressions. The node that is most preferred is
     the one with the greatest sum of weights, i.e. for each node that meets all
     of the scheduling requirements (resource request, requiredDuringScheduling
     affinity expressions, etc.), compute a sum by iterating through the
     elements of this field and adding "weight" to the sum if the node matches
     the corresponding matchExpressions; the node(s) with the highest sum are
     the most preferred.

     An empty preferred scheduling term matches all objects with implicit weight
     0 (i.e. it's a no-op). A null preferred scheduling term matches no objects
     (i.e. is also a no-op).

FIELDS:
   preference	<Object> -required-
   （优先级）
     A node selector term, associated with the corresponding weight.

   weight	<integer> -required-
   （权重1-100范围内，对于满足所有调度要求的每个节点，调度程序将通过迭代此字段的元素计算总和，并在节点与对应的节点匹配时将“权重”添加到总和。）
     Weight associated with matching the corresponding nodeSelectorTerm, in the
     range 1-100.

[root@adm-master1 home]# kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution
KIND:     Pod
VERSION:  v1

RESOURCE: requiredDuringSchedulingIgnoredDuringExecution <Object>

DESCRIPTION:
     If the affinity requirements specified by this field are not met at
     scheduling time, the pod will not be scheduled onto the node. If the
     affinity requirements specified by this field cease to be met at some point
     during pod execution (e.g. due to an update), the system may or may not try
     to eventually evict the pod from its node.

     A node selector represents the union of the results of one or more label
     queries over a set of nodes; that is, it represents the OR of the selectors
     represented by the node selector terms.

FIELDS:
   nodeSelectorTerms	<[]Object> -required-
     Required. A list of node selector terms. The terms are ORed.


[root@adm-master1 home]# kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms
KIND:     Pod
VERSION:  v1

RESOURCE: nodeSelectorTerms <[]Object>

DESCRIPTION:
     Required. A list of node selector terms. The terms are ORed.

     A null or empty node selector term matches no objects. The requirements of
     them are ANDed. The TopologySelectorTerm type implements a subset of the
     NodeSelectorTerm.

FIELDS:
   matchExpressions	<[]Object>
   （匹配表达式,这个标签可以指定一段，例如pod中定义的key为zone，operator为In(包含那些)，values为 foo和bar。就是在node节点中包含foo和bar的标签中调度）
     A list of node selector requirements by node's labels.

   matchFields	<[]Object>
   （匹配字段 和上面的意思 不过他可以不定义标签值，可以定义）
     A list of node selector requirements by node's fields.

```

硬亲和性:

```yaml
#yaml 文件：
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod
  labels:
    name: myapp
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: In
            values:
            - gomo1
            - gomo2

#创建并检测
[root@adm-master1 home]# kubectl create -f pod-affinity-demo.yaml -n gomo-test
pod/node-affinity-pod created
[root@adm-master1 home]# kubectl get pods  -n gomo-test
NAME                READY   STATUS    RESTARTS   AGE
node-affinity-pod   0/1     Pending   0          11s
[root@adm-master1 home]# kubectl describe pods node-affinity-pod  -n gomo-test
.....omit.....
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/7 nodes are available: 7 node(s) didn't match node selector.
  
  
[root@adm-master1 home]# kubectl get nodes --show-labels
NAME          STATUS   ROLES    AGE    VERSION   LABELS
...omit...
adm-worker2   Ready    <none>   2d5h   v1.17.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=adm-worker2,kubernetes.io/os=linux,zone=gomo2
#再次检测：
[root@adm-master1 home]# kubectl get pods -n gomo-test
NAME                READY   STATUS    RESTARTS   AGE
node-affinity-pod   1/1     Running   0          3m33s
```

 软亲和性：

与requiredDuringSchedulingIgnoredDuringExecution比较，这里需要注意的是preferredDuringSchedulingIgnoredDuringExecution是个列表项，而preference不是一个列表项了。

```yaml
#yaml 文件：
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod-2
  labels:
    name: myapp
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - gomo1
            - gomo2

#测试：
[root@adm-master1 home]# kubectl create -f pod-affinity-demo.yaml -n gomo-test
pod/node-affinity-pod-2 created
[root@adm-master1 home]# kubectl get pods -o wide -n gomo-test
NAME                  READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
node-affinity-pod-2   1/1     Running   0          14s   10.20.2.27   adm-worker2   <none>           <none>
```

### 13.2.3podAffinity

Pod亲和性场景，我们的k8s集群的节点分布在不同的区域或者不同的机房，当服务A和服务B要求部署在同一个区域或者同一机房的时候，我们就需要亲和性调度了

官方解释：

```
[root@adm-master1 manifests]# kubectl explain pod.spec.affinity.podAffinity
KIND:     Pod
VERSION:  v1

RESOURCE: podAffinity <Object>

DESCRIPTION:
     Describes pod affinity scheduling rules (e.g. co-locate this pod in the
     same node, zone, etc. as some other pod(s)).

     Pod affinity is a group of inter pod affinity scheduling rules.

FIELDS:
   preferredDuringSchedulingIgnoredDuringExecution	<[]Object>
     The scheduler will prefer to schedule pods to nodes that satisfy the
     affinity expressions specified by this field, but it may choose a node that
     violates one or more of the expressions. The node that is most preferred is
     the one with the greatest sum of weights, i.e. for each node that meets all
     of the scheduling requirements (resource request, requiredDuringScheduling
     affinity expressions, etc.), compute a sum by iterating through the
     elements of this field and adding "weight" to the sum if the node has pods
     which matches the corresponding podAffinityTerm; the node(s) with the
     highest sum are the most preferred.

   requiredDuringSchedulingIgnoredDuringExecution	<[]Object>
     If the affinity requirements specified by this field are not met at
     scheduling time, the pod will not be scheduled onto the node. If the
     affinity requirements specified by this field cease to be met at some point
     during pod execution (e.g. due to a pod label update), the system may or
     may not try to eventually evict the pod from its node. When there are
     multiple elements, the lists of nodes corresponding to each podAffinityTerm
     are intersected, i.e. all terms must be satisfied.

[root@adm-master1 manifests]# kubectl explain pod.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution
KIND:     Pod
VERSION:  v1

RESOURCE: preferredDuringSchedulingIgnoredDuringExecution <[]Object>

DESCRIPTION:
     The scheduler will prefer to schedule pods to nodes that satisfy the
     affinity expressions specified by this field, but it may choose a node that
     violates one or more of the expressions. The node that is most preferred is
     the one with the greatest sum of weights, i.e. for each node that meets all
     of the scheduling requirements (resource request, requiredDuringScheduling
     affinity expressions, etc.), compute a sum by iterating through the
     elements of this field and adding "weight" to the sum if the node has pods
     which matches the corresponding podAffinityTerm; the node(s) with the
     highest sum are the most preferred.

     The weights of all of the matched WeightedPodAffinityTerm fields are added
     per-node to find the most preferred node(s)

FIELDS:
   podAffinityTerm	<Object> -required-
     Required. A pod affinity term, associated with the corresponding weight.

   weight	<integer> -required-
     weight associated with matching the corresponding podAffinityTerm, in the
     range 1-100.

[root@adm-master1 manifests]# kubectl explain pod.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution
KIND:     Pod
VERSION:  v1

RESOURCE: requiredDuringSchedulingIgnoredDuringExecution <[]Object>

DESCRIPTION:
     If the affinity requirements specified by this field are not met at
     scheduling time, the pod will not be scheduled onto the node. If the
     affinity requirements specified by this field cease to be met at some point
     during pod execution (e.g. due to a pod label update), the system may or
     may not try to eventually evict the pod from its node. When there are
     multiple elements, the lists of nodes corresponding to each podAffinityTerm
     are intersected, i.e. all terms must be satisfied.

     Defines a set of pods (namely those matching the labelSelector relative to
     the given namespace(s)) that this pod should be co-located (affinity) or
     not co-located (anti-affinity) with, where co-located is defined as running
     on a node whose value of the label with key <topologyKey> matches that of
     any node on which a pod of the set of pods is running

FIELDS:
   labelSelector	<Object>
   （选择跟那组Pod亲和）
     A label query over a set of resources, in this case pods.

   namespaces	<[]string>
   （选择哪个命名空间亲和）
     namespaces specifies which namespaces the labelSelector applies to (matches
     against); null or empty list means "this pod's namespace"

   topologyKey	<string> -required-
   （指定节点上的哪个键）
     This pod should be co-located (affinity) or not co-located (anti-affinity)
     with the pods matching the labelSelector in the specified namespaces, where
     co-located is defined as running on a node whose value of the label with
     key topologyKey matches that of any node on which any of the selected pods
     is running. Empty topologyKey is not allowed.

```

硬亲和性：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod1
  labels:
    name: podaffinity-myapp
    tier: service
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
---
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod2
  labels:
    name: podaffinity-myapp
    tier: front
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: name
            operator: In
            values:
            - podaffinity-myapp
        topologyKey: kubernetes.io/hostname 
        
 
#创建并检测
[root@adm-master1 home]# kubectl create -f pod-affinity-demo.yaml -n gomo-test
pod/node-affinity-pod1 created
pod/node-affinity-pod2 created
[root@adm-master1 home]# kubectl get pods -n gomo-test -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
node-affinity-pod1   1/1     Running   0          14s   10.20.3.71   adm-worker1   <none>           <none>
node-affinity-pod2   1/1     Running   0          14s   10.20.3.72   adm-worker1   <none>           <none>
        
```

### 13.2.4podAntiAffinity

Pod反亲和性场景，当应用服务A和数据库服务B要求尽量不要在同一台节点上的时候。

 kubectl explain pod.spec.affinity.podAntiAffinity 也分为硬反亲和性和软反亲和性调度（和podAffinity一样的配置）

```yaml
#首先把两个node打上同一个标签。
[root@adm-master1 home]# kubectl label node adm-worker1 zone=gomo1
[root@adm-master1 home]# kubectl label node adm-worker2 zone=gomo1
[root@adm-master1 home]# kubectl label node adm-worker4 zone=gomo1
[root@adm-master1 home]# kubectl label node adm-worker3 zone=gomo1 

#反硬亲和调度
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod1
  labels:
    name: podaffinity-myapp
    tier: service
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
---
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod2
  labels:
    name: podaffinity-myapp
    tier: front
spec:
  containers:
  - name: myapp
    image: ikubernetes/myapp:v1
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: name
            operator: In
            values:
            - podaffinity-myapp
        topologyKey: zone
```

创建并检测：（因为zone这个key在每个node都有会，所以第二个Pod没有办法调度，所以一直Pending状态）

```
[root@adm-master1 home]# kubectl apply -f pod-affinity-demo.yaml -n gomo-test
pod/node-affinity-pod1 created
pod/node-affinity-pod2 created
[root@adm-master1 home]# kubectl get pods -o wide -n gomo-test
NAME                 READY   STATUS              RESTARTS   AGE   IP       NODE          NOMINATED NODE   READINESS GATES
node-affinity-pod1   0/1     ContainerCreating   0          3s    <none>   adm-worker1   <none>           <none>
node-affinity-pod2   0/1     Pending             0          3s    <none>   <none>        <none>           <none>
[root@adm-master1 home]# kubectl get pods -o wide -n gomo-test
NAME                 READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
node-affinity-pod1   1/1     Running   0          7s    10.20.3.76   adm-worker1   <none>           <none>
node-affinity-pod2   0/1     Pending   0          7s    <none>       <none>        <none>           <none>
```

### 13.2.5污点容忍调度（Taint和Toleration）

前两种方式都是pod选择那个pod，而污点调度是node选择的pod，污点就是定义在节点上的键值属性数据。举要作用是让节点拒绝pod，拒绝不合法node规则的pod。Taint（污点）和 Toleration（容忍）是相互配合的，可以用来避免 pod 被分配到不合适的节点上,每个节点上都可以应用**一个或多个** taint ，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。

- #### Taint

Taint是节点上属性，我们看一下Taints如何定义

kubectl explain node.spec.taints（对象列表）

- key 定义一个key
- value 定义一个值
- effect pod不能容忍这个污点时，他的行为是什么，行为分为三种：NoSchedule 仅影响调度过程，对现存的pod不影响。PreferNoSchedule 系统将*尽量*避免放置不容忍节点上污点的pod，但这不是必需的。就是软版的NoSchedule NoExecute 既影响调度过程，也影响现存的pod，不满足的pod将被驱逐。

```
kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N [options]
增加taint
kubectl  taint node k8s-node02 node-type=prod:NoSchedule
删除taint
kubectl  taint node k8s-node02 node-type:NoSchedule-
```

- #### tolerations

- key 被容忍的key
- tolerationSeconds 被驱逐的宽限时间，默认是0 就是立即被驱逐
- value 被容忍key的值
- operator Exists只要key在就可以调度，Equal（等值比较）必须是值要相同
- effect 节点调度后的操作

```yaml
#创建一个容忍
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      release: dev
  template:
    metadata:
      labels:
        app: myapp
        release: dev
    spec:
      containers:
      - name: myapp-containers
        image: ikubernetes/myapp:v2
        ports:
        - name: http
          containerPort: 80
      tolerations:
      - key: "node-type"
        operator: "Equal"
        value: "prod"
        effect: "NoSchedule"
```

## 13.4Pod优先级与抢占

Pod可以拥有优先级.优先意味着相对于其它pod某个pod更为重要.如果重要的pod不能被调度,则kubernetes调度器会优先于(驱离)低优先级的pod来让处于pending状态的高优先级pod被调度.

kubernetes 1.9以后,优先级会影响pod的调度顺序和资源耗尽时pod的驱离顺序

在1.11或以后的版本里,按下面指示来操作:

- 创建一个或者多个`PriorityClasses`
- 创建一个pod,并把`priorityClassName`设置为以上添加的`priorityClassName`中的一个.当然你不必直接创建pod,你可以把`priorityClassName`添加到集合对象(比如deployment)的template里.

如果你仅想尝试这项功能并且然后把它禁用,你必须把`PodPriority`设置为false,然后重启apiserver和调度器.被禁用以后,已经存在的pod仍然保留有它们的优先级字段,但是抢占不会再生效,并且优先字段被忽略.你也不能再向新的pod添加`priorityClassName`字段

### 13.4.1Pod优先级（Priority）

而pod priority指的是Pod的优先级，高优先级的Pod会优先被调度，或者在资源不足低情况牺牲低优先级的Pod，以便于重要的Pod能够得到资源部署。

为了定义Pod优先级，需要先定义PriorityClass对象，该对象没有Namespace限制，官网示例：

```yaml
apiVersion: scheduling.k8s.io/v1alpha1
kind: PriorityClass
metadata:
name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
```

然后通过在Pod的spec. priorityClassName中指定已定义的PriorityClass名称即可：

```yaml
apiVersion: v1
kind: Pod
metadata:
name: nginx
labels:
env: test
spec:
containers:
- name: nginx
image: nginx
imagePullPolicy: IfNotPresent
priorityClassName: high-priority

```



### 13.4.2抢占（Preemption）

当节点没有足够的资源供调度器调度Pod、导致Pod处于pending时，抢占（preemption）逻辑会被触发。Preemption会尝试从一个节点删除低优先级的Pod，从而释放资源使高优先级的Pod得到节点资源进行部署。

怎样禁用抢占？

在kubernetes1.11以后版本,抢占被kube-scheduler的的`disablePreemption`标识控制,默认设置是false.如果你不顾建议仍然想要禁用它,可以把它设置为true

这个选项仅仅在组件的配置里有效,在旧式的命令行下无效.下面是一个配置的示例:

```yaml
apiVersion: componentconfig/v1alpha1
kind: KubeSchedulerConfiguration
algorithmSource:
  provider: DefaultProvider

...

disablePreemption: true
```

