## 9、	Kubernetes控制器

### 9.1RC 和 RS

#### 9.1.1 Replication Controller 

Replication Controller 简称 RC ， RC 是 Kubernetes 系统中的核⼼概念之⼀，简单来说， RC 可以保证在任意时间运⾏ Pod 的副本数量，能够保证 Pod 总是可⽤的。如果实际 Pod 数量⽐指定的多那 就结束掉多余的，如果实际数量⽐指定的少就新启动⼀些 Pod ，当 Pod 失败、被删除或者挂掉 后， RC 都会去⾃动创建新的 Pod 来保证副本数量，所以即使只有⼀个 Pod ，我们也应该使⽤ RC 来 管理我们的 Pod 。

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-test
  namespace: gomo-test
  labels:
    name: rc
spec:
  replicas: 3
  selector:
    name: rc
  template:
    metadata:
      labels:
        name: rc
    spec:
      containers:
      - name: nginx-test
        image: nginx
        ports:
        - containerPort: 80

```



上⾯的 YAML ⽂件相对于我们之前的 Pod 的格式： 

- kind： ReplicationController

- spec.replicas: 指定 Pod 副本数量，默认为1

- spec.selector: RC 通过该属性来筛选要控制的 Pod

- spec.template: 这⾥就是我们之前的 Pod 的定义的模块，但是不需要 apiVersion 和 kind 了

- spec.template.metadata.labels: 注意这⾥的 Pod 的 labels 要和 spec.selector 相同，这样 RC 就可以来控制当前这个 Pod 了。

  详细使用explain 来查看RC的全部细节

  ```
  [root@adm-master1 ~]# kubectl explain rc
  ```

  

  这个 YAML ⽂件中的意思就是定义了⼀个 RC 资源对象，它的名字叫 rc-demo ，保证⼀直会有3 个 Pod 运⾏， Pod 的镜像是 nginx 镜像。

------

注意 spec.selector 和 spec.template.metadata.labels 这两个字段必须相同，否则会创建失败的，当然我们也可以不写 spec.selector ，这样就默认与 Pod 模板中的 metadata.labels 相同了。所以为了避免不必要的错误的话，不写为好。

------

然后我们来创建上⾯的 RC 对象(保存为 test.yaml):

```
[root@adm-master1 home]# kubectl create -f test.yaml 
replicationcontroller/rc-test created

```

查看 RC ：

```
[root@adm-master1 home]# kubectl get rc -n gomo-test
NAME      DESIRED   CURRENT   READY   AGE
rc-test   3         3         3       29s

```

查看具体信息：

```
[root@adm-master1 home]# kubectl describe rc rc-test -n gomo-test
Name:         rc-test
Namespace:    gomo-test
Selector:     name=rc
Labels:       name=rc
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  name=rc
  Containers:
   nginx-test:
    Image:        nginx
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  58s   replication-controller  Created pod: rc-test-wg6f7
  Normal  SuccessfulCreate  58s   replication-controller  Created pod: rc-test-p5wjk
  Normal  SuccessfulCreate  58s   replication-controller  Created pod: rc-test-45skr

或者
[root@adm-master1 home]# kubectl edit rc rc-test -n gomo-test
```

⽽且我们还可以⽤ RC 来进⾏滚动升级，⽐如我们将镜像地址更改为 nginx:1.7.9 :(**老版本**)

```
$ kubectl rolling-update rc-demo --image=nginx:1.7.9
```

但是如果我们的 Pod 中多个容器的话，就需要通过修改 YAML ⽂件来进⾏修改了:

```
 $ kubectl rolling-update rc-demo -f rc-demo.yaml
```

如果升级完成后出现了新的问题，想要⼀键回滚到上⼀个版本的话，使⽤ RC 只能⽤同样的⽅法把镜 像地址替换成之前的，然后重新滚动升级。

新版本 rolling-update 被弃用 改成rollout：

```
[root@adm-master1 home]# kubectl rollout --help
Manage the rollout of a resource.
  
 Valid resource types include:

  *  deployments
  *  daemonsets
  *  statefulsets

Examples:
  # Rollback to the previous deployment
  kubectl rollout undo deployment/abc
  
  # Check the rollout status of a daemonset
  kubectl rollout status daemonset/foo

Available Commands:
  history     View rollout history
  pause       Mark the provided resource as paused
  restart     Restart a resource
  resume      Resume a paused resource
  status      Show the status of the rollout
  undo        Undo a previous rollout

Usage:
  kubectl rollout SUBCOMMAND [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
```

#### 9.1.2Replication Set（RS）

Replication Set 简称 RS ，随着 Kubernetes 的⾼速发展，官⽅已经推荐我们使 ⽤ RS 和 Deployment 来代替 RC 了，实际上 RS 和 RC 的功能基本⼀致，⽬前唯⼀的⼀个区别就 是 RC 只⽀持基于等式的 selector （env=dev或environment!=qa），但 RS 还⽀持基于集合 的 selector （version in (v1.0, v2.0)），这对复杂的运维管理就⾮常⽅便了

kubectl 命令⾏⼯具中关于 RC 的⼤部分命令同样适⽤于我们的 RS 资源对象。不过我们也很少会去 单独使⽤ RS ，它主要被 Deployment 这个更加⾼层的资源对象使⽤，除⾮⽤户需要⾃定义升级功能或 根本不需要升级 Pod ，在⼀般情况下，我们推荐使⽤ Deployment ⽽不直接使⽤ Replica Set 。

最后我们总结下关于 RC / RS 的⼀些特性和作⽤吧：

1. ⼤部分情况下，我们可以通过定义⼀个 RC 实现的 Pod 的创建和副本数量的控制
2.  RC 中包含⼀个完整的 Pod 定义模块（不包含 apiversion 和 kind ）
3.  RC 是通过 label selector 机制来实现对 Pod 副本的控制的 
4. 通过改变 RC ⾥⾯的 Pod 副本数量，可以实现 Pod 的扩缩容功能 
5. 通过改变 RC ⾥⾯的 Pod 模板中镜像版本，可以实现 Pod 的滚动升级功能（但是不⽀持⼀键回 滚，需要⽤相同的⽅法去修改镜像地址）

### 9.2 Deployment

Deployment 同样也是 Kubernetes 系统的⼀个核⼼概念，主要职责和 RC ⼀样的都是保证 Pod 的数量 和健康，⼆者⼤部分功能都是完全⼀致的，我们可以看成是⼀个升级版的 RC 控制器， 那 Deployment ⼜具备那些新特性呢？

1. RC 的全部功能： Deployment 具备上⾯描述的 RC 的全部功
2. 事件和状态查看：可以查看 Deployment 的升级详细进度和状态 
3. 回滚：当升级 Pod 的时候如果出现问题，可以使⽤回滚操作回滚到之前的任⼀版本 
4. 版本记录：每⼀次对 Deployment 的操作，都能够保存下来，这也是保证可以回滚到任⼀版本的基础 
5. 暂停和启动：对于每⼀次升级都能够随时暂停和启动

作为对⽐，我们知道 Deployment 作为新⼀代的 RC ，不仅在功能上更为丰富了，同时现在官⽅也都是推荐使用Deployment 来管理 Pod 的，⽐如⼀些官⽅组件 kube-dns ，kube-proxy 也都是 使⽤的 Deployment 来管理的，所以当⼤家在使⽤的使⽤也最好使⽤ Deployment 来管理 Pod 。

![image-20191231163642189](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20191231163642189.png)



可以看出⼀个Deployment拥有多个Replica Set，⽽⼀个Replica Set拥有⼀个或多个Pod。⼀个Deployment控制多个rs主要是为了⽀持回滚机制，每当Deployment操作时，Kubernetes会重新⽣成⼀个Replica Set并保留，以后有需要的话就可以回滚⾄之前的状态。 下⾯创建⼀个Deployment，它创建了⼀个Replica Set来启动3个nginx pod，yaml⽂件如下：

（**老版本**）

```
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    k8s-app: nginx-demo
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx  
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

在新版本中会报错：

```
[root@adm-master1 home]# kubectl create -f test.yaml 
error: unable to recognize "test.yaml": no matches for kind "Deployment" in version "apps/v1beta1"
```

原因是因为 自从1.16 +之后 k8s 废弃了deploy的”apps/v1beta1“的API  改成了 apps/v1  我们不能自己的单独的只修改apps/beta1  2019.12月截至 提供了2种方式 

1.  配合官网使用新的deploy的apps/v1写法
2.  使用官方提供的转换功能（推荐！）

```
kubectl convert -f ./my-deployment.yaml --output-version apps/v1
```

转化之后的：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    k8s-app: nginx-demo
  name: nginx-deploy
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.7.9
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

```

```
[root@adm-master1 home]# kubectl create -f new-deploy.yml 
deployment.apps/nginx-deploy created
```

然后执⾏⼀下命令查看刚刚创建的Deployment:

```
[root@adm-master1 home]# kubectl get deployments
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deploy     3/3     3            3           63s
我们可以看到Deployment已经创建了1个Replica Set了，执⾏下⾯的命令查看rs和pod
[root@adm-master1 home]# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
nginx-deploy-54f57cf6bf     3         3         3       114s

[root@adm-master1 home]# kubectl get pod --show-labels
NAME                             READY   STATUS    RESTARTS   AGE     LABELS
nginx-deploy-54f57cf6bf-njjvl    1/1     Running   0          2m27s   app=nginx,pod-template-hash=54f57cf6bf
nginx-deploy-54f57cf6bf-p6qwt    1/1     Running   0          2m26s   app=nginx,pod-template-hash=54f57cf6bf
nginx-deploy-54f57cf6bf-rr66q    1/1     Running   0          2m26s   app=nginx,pod-template-hash=54f57cf6bf
```

上⾯的Deployment的yaml⽂件中的 replicas:3 将会保证我们始终有3个POD在运⾏。 由于 Deployment 和 RC 的功能⼤部分都⼀样的 这⾥重点给⼤家演示下 Deployment 的滚动升级和回滚功能。

现在我们将刚刚保存的yaml⽂件中的nginx镜像修改为 nginx:1.16.0

```
spec:
      containers:
      - image: nginx:1.16.0
        imagePullPolicy: IfNotPresent
```

```
[root@adm-master1 home]# kubectl rollout status deployment/nginx-deploy
deployment "nginx-deploy" successfully rolled out

[root@adm-master1 home]# kubectl rollout history deployment/nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

暂停升级
[root@adm-master1 home]# kubectl rollout pause deployment/nginx-deploy


继续升级
[root@adm-master1 home]# kubectl rollout resume deployment/nginx-deploy

升级结束后，继续查看rs的状态：
[root@adm-master1 home]# kubectl get rs
NAME                        DESIRED   CURRENT   READY   AGE
nginx-deploy-54f57cf6bf     0         0         0       12m
nginx-deploy-76d59f54b8     3         3         3       4m15s



[root@adm-master1 home]# kubectl describe pods nginx-deploy-76d59f54b8-c7mgt
......omit....
  Type    Reason     Age        From                  Message
  ----    ------     ----       ----                  -------
  Normal  Scheduled  <unknown>  default-scheduler     Successfully assigned default/nginx-deploy-76d59f54b8-c7mgt to adm-worker2
  Normal  Pulling    4m39s      kubelet, adm-worker2  Pulling image "nginx:1.16.0"
  Normal  Pulled     4m29s      kubelet, adm-worker2  Successfully pulled image "nginx:1.16.0"
  Normal  Created    4m29s      kubelet, adm-worker2  Created container nginx
  Normal  Started    4m29s      kubelet, adm-worker2  Started container nginx


现在要直接回退到当前版本的前⼀个版本：
[root@adm-master1 home]# kubectl rollout undo deployment nginx-deploy
deployment.apps/nginx-deploy rolled back

当然也可以⽤ revision 回退到指定的版本：
[root@adm-master1 home]# kubectl rollout history deployment nginx-deploy
deployment.apps/nginx-deploy 
REVISION  CHANGE-CAUSE
2         <none>
3         <none>
[root@adm-master1 home]# kubectl rollout undo deployment nginx-deploy --to-revision=2
deployment.apps/nginx-deploy rolled back


```

### 9.3 Daemonset

通过该控制器的名称我们可以看出它的⽤法：Daemon，就是⽤来部署守护进程的， DaemonSet ⽤于 在每个 Kubernetes 节点中将守护进程的副本作为后台进程运⾏，说⽩了就是在每个节点部署⼀ 个 Pod 副本，当节点加⼊到 Kubernetes 集群中， Pod 会被调度到该节点上运⾏，当节点从集群只能 够被移除后，该节点上的这个 Pod 也会被移除，当然，如果我们删除 DaemonSet ，所有和这个对象相 关的 Pods 都会被删除。

在哪种情况下我们会需要⽤到这种业务场景呢？其实这种场景还是⽐较普通的，⽐如：

- 集群存储守护程序，如 glusterd 、 ceph 要部署在每个节点上以提供持久性存储； 
- 节点监视守护进程，如 Prometheus 监控集群，可以在每个节点上运⾏⼀个 node-exporter 进程来 收集监控节点的信息； 
- ⽇志收集守护程序，如 fluentd 或 logstash ，在每个节点上运⾏以收集容器的⽇志

这⾥需要特别说明的⼀个就是关于 DaemonSet 运⾏的 Pod 的调度问题，正常情况下， Pod 运⾏在哪 个节点上是由 Kubernetes 的调度器策略来决定的，然⽽，由 DaemonSet 控制器创建的 Pod 实际上提 前已经确定了在哪个节点上了（ Pod 创建时指定了 .spec.nodeName ），所以： DaemonSet 并不关⼼⼀个节点的 unshedulable 字段， DaemonSet 可以创建 Pod ，即使调度器还没有启动，这点⾮常重要。

使用yaml文件 创建一个ds：（老版本 的daemonset api 为extensions/v1beta1  自1.16+之后 新版本的 为apps/v1 转换方式和deploy 一样）

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
  selector:
    matchLabels:
      app: nginx-ds
```

```
然后直接创建即可：
$ kubectl create -f nginx-ds.yaml



然后我们可以观察下 Pod 是否被分布到了每个节点上：
[root@adm-master1 home]# kubectl get nodes
NAME          STATUS   ROLES    AGE     VERSION
adm-master1   Ready    master   5h44m   v1.17.0
adm-master2   Ready    master   5h35m   v1.17.0
adm-master3   Ready    master   5h37m   v1.17.0
adm-worker1   Ready    <none>   5h37m   v1.17.0
adm-worker2   Ready    <none>   5h37m   v1.17.0
adm-worker3   Ready    <none>   5h37m   v1.17.0
adm-worker4   Ready    <none>   5h37m   v1.17.0

[root@adm-master1 home]# kubectl get pods  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP           NODE          NOMINATED NODE   READINESS GATES
nginx-ds-6p2dg                   1/1     Running   0          3h27m   10.20.2.7    adm-worker2   <none>           <none>
nginx-ds-hdqzs                   1/1     Running   0          3h27m   10.20.1.8    adm-worker3   <none>           <none>
nginx-ds-k4f9t                   1/1     Running   0          3h27m   10.20.3.10   adm-worker1   <none>           <none>
nginx-ds-sctjn                   1/1     Running   0          3h27m   10.20.4.8    adm-worker4   <none>           <none>

```

### 9.4 StatefuleSet

在学习 StatefulSet 这种控制器之前，我们就得先弄明⽩⼀个概念：什么是有状态服务？什么是⽆状 态服务？

- ⽆状态服务（Stateless Service）：该服务运⾏的实例不会在本地存储需要持久化的数据，并且多 个实例对于同⼀个请求响应的结果是完全⼀致的，⽐如前⾯我们讲解的 WordPress 实例，我们是 不是可以同时启动多个实例，但是我们访问任意⼀个实例得到的结果都是⼀样的吧？因为他唯⼀ 需要持久化的数据是存储在 MySQL 数据库中的，所以我们可以说 WordPress 这个应⽤是⽆状态服 务，但是 MySQL 数据库就不是了，因为他需要把数据持久化到本地。 
- 有状态服务（Stateful Service）：就和上⾯的概念是对⽴的了，该服务运⾏的实例需要在本地存 储持久化数据，⽐如上⾯的 MySQL 数据库，你现在运⾏在节点A，那么他的数据就存储在节点A上 ⾯的，如果这个时候你把该服务迁移到节点B去的话，那么就没有之前的数据了，因为他需要去对 应的数据⽬录⾥⾯恢复数据，⽽此时没有任何数据。

⽐如我们常⻅的 WEB 应⽤，是通过 session 来保持⽤户的登录状态的，如果我们将 session 持久化到节点上，那么该应⽤就是⼀个有状态的服务了，因为我现在登录进来你把我的 session 持久化到节点A上了，下次我登录的时候可能会将请求路由到节点B上去了，但是节点B上根本就没有我当前的 session 数据，就会被认为是未登录状态了，这样就导致我前后两次请求得到的结果不⼀致了。所以⼀般为了横向扩展，我们都会把这类 WEB 应⽤改成⽆状态的服务将 session 数据存⼊⼀个公共的地⽅，⽐如 redis ⾥⾯，是不是就可以了，对于⼀些客户端请求 API 的情况，我们就不使⽤ session 来保持⽤户状态，改成⽤ token 也是可以的。

⽆状态服务利⽤我们前⾯的 Deployment 或者 RC 都可以很好的控制，对应有状态服务，需要考虑的细 节就要多很多了，容器化应⽤程序最困难的任务之⼀，就是设计有状态分布式组件的部署体系结构。 由于⽆状态组件可能没有预定义的启动顺序、集群要求、点对点 TCP 连接、唯⼀的⽹络标识符、正常 的启动和终⽌要求等，因此可以很容易地进⾏容器化。诸如数据库，⼤数据分析系统，分布式 key/value 存储和 message brokers 可能有复杂的分布式体系结构，都可能会⽤到上述功能。为 此， Kubernetes 引⼊了 StatefulSet 资源来⽀持这种复杂的需求。

StatefulSet 类似于 ReplicaSet ，但是它可以处理 Pod 的启动顺序，为保留每个 Pod 的状态设置唯 ⼀标识，同时具有以下功能：

- 稳定的、唯⼀的⽹络标识符 
- 稳定的、持久化的存储 
- 有序的、优雅的部署和缩放 
- 有序的、优雅的删除和终⽌ 
- 有序的、⾃动滚动更新



创建StatefulSet

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: statefulset-test1
  labels:
    release: stable
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /home/sset-test

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: statefulset-test2
  labels:
    release: stable
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /home/sset-test
  
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
    role: stateful
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  creationTimestamp: null
  labels:
    app: nginx
    role: stateful
  name: web
spec:
  podManagementPolicy: OrderedReady
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
      role: stateful
  serviceName: nginx
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
        role: stateful
    spec:
      containers:
      - image: cnych/nginx-slim:0.8
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 80
          name: web
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: www
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  updateStrategy:
    type: OnDelete
  volumeClaimTemplates:
  - metadata:
      creationTimestamp: null
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      volumeMode: Filesystem
    status:
      phase: Pending
status:
  replicas: 0

```



在⼀个终端中，使⽤ kubectl create 来创建定义在 statefulset-demo.yaml 中的 Headless Service 和StatefulSet。

```
[root@adm-master1 home]# kubectl create -f gomo-statefulset.yaml 
persistentvolume/statefulset-test1 created
persistentvolume/statefulset-test2 created
service/nginx created
statefulset.apps/web created
```

在⼀个终端中，使⽤ kubectl get 来查看 StatefulSet 的 Pods 的创 建情况。

```
[root@adm-master1 home]# kubectl get pods -w -l role=stateful
NAME    READY   STATUS    RESTARTS   AGE
web-0   0/1     Pending   0          0s
web-0   0/1     Pending   0          0s
web-0   0/1     ContainerCreating   0          0s
web-0   1/1     Running             0          3s
web-1   0/1     Pending             0          0s
web-1   0/1     Pending             0          0s
web-1   0/1     ContainerCreating   0          0s
web-1   1/1     Running             0          2s


#最终结果
[root@adm-master1 home]# kubectl get pods -w -l role=stateful
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          2m31s
web-1   1/1     Running   0          2m28s

```

如同 StatefulSets 概念中所提到的， StatefulSet 中的 Pod 拥有⼀个具有稳定的、独⼀⽆⼆的身份标志。这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯⼀顺序索引。 Pod 的名称的形式为 <statefulset name>-<ordinal index> 。web StatefulSet 拥有两个副本，所以它创建了两个 Pod：web-0 和 web-1。上⾯的命令创建了两个 Pod，每个都运⾏了⼀个 NGINX web 服务器。获取 nginx Service 和 web StatefulSet 来验证是否成功的创建了它们。

```
[root@adm-master1 home]# kubectl get service nginx
NAME    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx   ClusterIP   None         <none>        80/TCP    4m28s
[root@adm-master1 home]# kubectl get statefulset web
NAME   READY   AGE
web    2/2     4m38s
```

#### 9.4.1 headless service

headless service 是将service的发布文件中的clusterip=none ，不让其获取clusterip ， DNS解析的时候直接走pod

第一种：自主选择权，有时候`client`想自己来决定使用哪个`Real Server`，可以通过查询`DNS`来获取`Real Server`的信息。

第二种：`Headless Services`还有一个用处（PS：也就是我们需要的那个特性）。`Headless Service`的对应的每一个`Endpoints`，即每一个`Pod`，都会有对应的`DNS`域名；这样`Pod`之间就可以互相访问。





```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: headless-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  clusterIP: None
```

创建 并检测：

```
[root@adm-master1 home]# kubectl create -f gomo-headless.yaml 
deployment.apps/nginx-deployment created
service/headless-service created
[root@adm-master1 home]# kubectl get svc
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
headless-service   ClusterIP   None            <none>        80/TCP         8s
```

检测：

```
# 进入一个测试的pod的里面 ping 无头服务的时候 解析的是pod IP
/ # ping headless-service.default.svc.cluster.local
PING headless-service.default.svc.cluster.local (10.20.3.26): 56 data bytes
64 bytes from 10.20.3.26: seq=0 ttl=64 time=0.379 ms
64 bytes from 10.20.3.26: seq=1 ttl=64 time=0.153 ms
64 bytes from 10.20.3.26: seq=2 ttl=64 time=0.205 ms
--- headless-service.default.svc.cluster.local ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.153/0.245/0.379 ms

#但是如果ping 的是某一个service的话 解析出来的地址就是clusterip
```

### 9.5 Job

Job，我们在⽇常的⼯作中经常都会遇到⼀些需要进⾏批量数据处理和分析的需求，当然也会有按时间来进⾏调度的⼯作，在我们的 Kubernetes 集群中为我们提供了 Job 和 CronJob 两种资源对象来应对我们的这种需求。Job 负责处理任务，即仅执⾏⼀次的任务，它保证批处理任务的⼀个或多个 Pod 成功结束。⽽ CronJob 则就是在 Job 上加上了时间调度。

我们⽤ Job 这个资源对象来创建⼀个任务，我们定⼀个 Job 来执⾏⼀个倒计时的任务，定义 YAML ⽂ 件：

```
apiVersion: batch/v1
kind: Job
metadata:
  name: job-demo
spec:
  template:
    metadata:
      name: job-demo
    spec:
      restartPolicy: Never
      containers:
      - name: counter
        image: busybox
        command:
        - "bin/sh"
        - "-c"
        - "for i in 9 8 7 6 5 4 3 2 1; do echo $i; done"
```

注意 Job 的 RestartPolicy 仅⽀持 Never 和 OnFailure 两种，不⽀持 Always ，我们知道 Job 就相 当于来执⾏⼀个批处理任务，执⾏完就结束了，如果⽀持 Always 的话是不是就陷⼊了死循环了？ 然后来创建该 Job 

```
[root@adm-master1 home]# kubectl create -f gomo-job.yaml 
job.batch/job-demo created
[root@adm-master1 home]# kubectl get job
NAME                      COMPLETIONS   DURATION   AGE
cronjob-demo-1577932140   1/1           3s         2m58s
cronjob-demo-1577932200   1/1           2s         117s
cronjob-demo-1577932260   1/1           2s         57s
job-demo                  1/1           3s         11s
[root@adm-master1 home]# kubectl get pods 
NAME                             READY   STATUS              RESTARTS   AGE
job-demo-9hr8g                   0/1     Completed           0          76s
[root@adm-master1 home]# kubectl logs job-demo-9hr8g
9
8
7
6
5
4
3
2
1
```

### 9.6 CronJob

CronJob 其实就是在 Job 的基础上加上了时间调度，我们可以：在给定的时间点运⾏⼀个任务，也可 以周期性地在给定时间点运⾏。这个实际上和我们 Linux 中的 crontab 就⾮常类似了。 ⼀个 CronJob 对象其实就对应中 crontab ⽂件中的⼀⾏，它根据配置的时间格式周期性地运⾏⼀ 个 Job ，格式和 crontab 也是⼀样的。

 crontab 的格式如下： 

```
分 时 ⽇ ⽉ 星期 要运⾏的命令
第1列分钟0～59 第2列⼩时0～23） 第3列⽇1～31 第4列⽉1～ 12 第5列星期0～7（0和7表示星期天） 第6列要运⾏的命令
```

现在，我们⽤ CronJob 来管理我们上⾯的 Job 任务，

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: cronjob-demo
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: hello
            image: busybox
            args:
            - "bin/sh"
            - "-c"
            - "for i in 9 8 7 6 5 4 3 2 1; do echo $i; done"
```

(在老版本 cronjob的apiversion 为：batch/v2alpha1   新版本为batch/v1beta1) 

```
[root@adm-master1 home]# kubectl create -f test.yaml 
cronjob.batch/cronjob-demo created

[root@adm-master1 home]# kubectl get cronjob
NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob-demo   */1 * * * *   False     1        8s              23s
[root@adm-master1 home]# kubectl get pods
NAME                             READY   STATUS      RESTARTS   AGE
cronjob-demo-1577933460-4zddg    0/1     Completed   0          57s
```

我们这⾥的 Kind 是 CronJob 了，要注意的是 .spec.schedule 字段是必须填写的，⽤来指定任务运⾏ 的周期，格式就和 crontab ⼀样，另外⼀个字段是 .spec.jobTemplate , ⽤来指定需要运⾏的任务，格 式当然和 Job 是⼀致的。还有⼀些值得我们关注的字 段 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit ，表示历史限制，是可选的 字段。它们指定了可以保留多少完成和失败的 Job ，默认没有限制，所有成功和失败的 Job 都会被保 留。然⽽，当运⾏⼀个 Cron Job 时， Job 可以很快就堆积很多，所以⼀般推荐设置这两个字段的 值。如果设置限制的值为 ，那么相关类型的 完成后将不会被保留。

⼀旦 Job 被删除，由 Job 创建的 Pod 也会被删除。注意，所有由名称为 “hello” 的 Cron Job 创建的 Job 会以前缀字符串 “hello-” 进⾏命名。如果想要删除当前 Namespace 中的所有 Job，可以通过命令 kubectl delete jobs --all ⽴刻删除它们。

```
kubectl delete job job-demo
kubectl get cronjob
kubectl delete cronjob cronjob-demo
kubectl get  pods 
```



### 9.7 Admission Controller （准入控制器）

准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：`MutatingAdmissionWebhook` 和 `ValidatingAdmissionWebhook`。分别作为配置的变异和验证[准入控制 webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)。

**变更（Mutating）准入控制**：修改请求的对象

**验证（Validating）准入控制**：验证请求的对象

准入控制器是在 API Server 的启动参数重配置的。一个准入控制器可能属于以上两者中的一种，也可能两者都属于。当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。

我们在部署 Kubernetes 集群的时候都会默认开启一系列准入控制器，如果没有设置这些准入控制器的话可以说你的 Kubernetes 集群就是在裸奔，应该只有集群管理员可以修改集群的准入控制器。

例如我会默认开启如下的准入控制器。

```bash
--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota,MutatingAdmissionWebhook,ValidatingAdmissionWebhook
Copy
```

准入控制器列表

Kubernetes 目前支持的准入控制器有：

- **AlwaysPullImages**：此准入控制器修改每个 Pod 的时候都强制重新拉取镜像。
- **DefaultStorageClass**：此准入控制器观察创建`PersistentVolumeClaim`时不请求任何特定存储类的对象，并自动向其添加默认存储类。这样，用户就不需要关注特殊存储类而获得默认存储类。
- **DefaultTolerationSeconds**：此准入控制器将Pod的容忍时间`notready:NoExecute`和`unreachable:NoExecute` 默认设置为5分钟。
- **DenyEscalatingExec**：此准入控制器将拒绝`exec` 和附加命令到以允许访问宿主机的升级了权限运行的pod。
- **EventRateLimit (alpha)**：此准入控制器缓解了 API Server 被事件请求淹没的问题，限制时间速率。
- **ExtendedResourceToleration**：此插件有助于创建具有扩展资源的专用节点。
- **ImagePolicyWebhook**：此准入控制器允许后端判断镜像拉取策略，例如配置镜像仓库的密钥。
- **Initializers (alpha)**：Pod初始化的准入控制器，详情请参考[动态准入控制](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/)。
- **LimitPodHardAntiAffinityTopology**：此准入控制器拒绝任何在 `requiredDuringSchedulingRequiredDuringExecution` 的 `AntiAffinity` 字段中定义除了`kubernetes.io/hostname` 之外的拓扑关键字的 pod 。
- **LimitRanger**：此准入控制器将确保所有资源请求不会超过 namespace 的 `LimitRange`。
- **MutatingAdmissionWebhook （1.9版本中为beta）**：该准入控制器调用与请求匹配的任何变更 webhook。匹配的 webhook是串行调用的；如果需要，每个人都可以修改对象。
- **NamespaceAutoProvision**：此准入控制器检查命名空间资源上的所有传入请求，并检查引用的命名空间是否存在。如果不存在就创建一个命名空间。
- **NamespaceExists**：此许可控制器检查除 `Namespace` 其自身之外的命名空间资源上的所有请求。如果请求引用的命名空间不存在，则拒绝该请求。
- **NamespaceLifecycle**：此准入控制器强制执行正在终止的命令空间中不能创建新对象，并确保`Namespace`拒绝不存在的请求。此准入控制器还防止缺失三个系统保留的命名空间`default`、`kube-system`、`kube-public`。
- **NodeRestriction**：该准入控制器限制了 kubelet 可以修改的`Node`和`Pod`对象。
- **OwnerReferencesPermissionEnforcement**：此准入控制器保护对`metadata.ownerReferences`对象的访问，以便只有对该对象具有“删除”权限的用户才能对其进行更改。
- **PodNodeSelector**：此准入控制器通过读取命名空间注释和全局配置来限制可在命名空间内使用的节点选择器。
- **PodPreset**：此准入控制器注入一个pod，其中包含匹配的PodPreset中指定的字段，详细信息见[Pod Preset](https://rootsongjc.gitbooks.io/kubernetes-handbook/concepts/pod-preset.html)。
- **PodSecurityPolicy**：此准入控制器用于创建和修改pod，并根据请求的安全上下文和可用的Pod安全策略确定是否应该允许它。
- **PodTolerationRestriction**：此准入控制器首先验证容器的容忍度与其命名空间的容忍度之间是否存在冲突，并在存在冲突时拒绝该容器请求。
- **Priority**：此控制器使用`priorityClassName`字段并填充优先级的整数值。如果未找到优先级，则拒绝Pod。
- **ResourceQuota**：此准入控制器将观察传入请求并确保它不违反命名空间的`ResourceQuota`对象中列举的任何约束。
- **SecurityContextDeny**：此准入控制器将拒绝任何试图设置某些升级的[SecurityContext](https://kubernetes.io/docs/user-guide/security-context)字段的pod 。
- **ServiceAccount**：此准入控制器实现[serviceAccounts的](https://kubernetes.io/docs/user-guide/service-accounts)自动化。
- **用中的存储对象保护**：该`StorageObjectInUseProtection`插件将`kubernetes.io/pvc-protection`或`kubernetes.io/pv-protection`终结器添加到新创建的持久卷声明（PVC）或持久卷（PV）。在用户删除PVC或PV的情况下，PVC或PV不会被移除，直到PVC或PV保护控制器从PVC或PV中移除终结器。有关更多详细信息，请参阅使用中的[存储对象保护](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#storage-object-in-use-protection)。
- **ValidatingAdmissionWebhook（1.8版本中为alpha；1.9版本中为beta）**：该准入控制器调用与请求匹配的任何验证webhook。匹配的webhooks是并行调用的；如果其中任何一个拒绝请求，则请求失败。

Kubernetes 1.10之前的版本可以使用 `--admission-control` 打开 Admission Controller。同时 `--admission-control` 的顺序决定 Admission 运行的先后。其实这种方式对于用户来讲其实是挺复杂的，因为这要求用户对所有的 Admission Controllers 需要完全了解。

如果使用Kubernetes 1.10之后的版本， `--admission-control` 已经废弃，建议使用 `--enable-admission-plugins --disable-admission-plugins` 指定需要打开或者关闭的 Admission Controller。 同时用户指定的顺序并不影响实际 Admission Controllers 的执行顺序，对用户来讲非常友好。

值得一提的是，有些 Admission Controller 可能会使用 Alpha 版本的 API，这时必须首先使能其使用的 API 版本。否则 Admission Controller 不能工作，可能会影响系统功能。



### 9.8 HPA

Pod ⾃动扩缩容

过通过⼿⼯执⾏ kubectl scale 命令和在 Dashboard 上操作可以实现 Pod 的扩缩容，但是这样毕竟需要每次去⼿⼯操作⼀次，⽽且指不定什么时候业务请求量就很⼤了，所以如果不能做到⾃动化的去扩缩容的话，这也是⼀个很麻烦的事情。如果 Kubernetes 系统能够根据 Pod 当前的负载的变化情况来⾃动的进⾏扩缩容就好了，因为这个过程本来就是不固定的，频繁发⽣的，所以纯⼿⼯的⽅式不是很现实。Horizontal Pod Autoscaling （Pod⽔平⾃动伸缩），简称 HPA 。 HAP 通过监控分析 RC 或者 Deployment 控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量，这是 HPA 最基本的原理。

![image-20200102112521715](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20200102112521715.png)

kubectl autoscale deploy mytomcat --max=5 --min=3 --cpu-percent=80

kubectl delete horizontalpodautoscaler.autoscaling/mytomcat

HPA 在 kubernetes 集群中被设计成⼀个 controller ，我们可以简单的通过 kubectl autoscale 命令 来创建⼀个 HPA 资源对象， HPA Controller 默认30s轮询⼀次（可通过 kube-controller-manager 的 标志 --horizontal-pod-autoscaler-sync-period 进⾏设置），查询指定的资源（RC或者 Deployment）中 Pod 的资源使⽤率，并且与创建时设定的值和指标做对⽐，从⽽实现⾃动伸缩的功 能。当你创建了 HPA 后， HPA 会从 Heapster 或者⽤户⾃定义的 RESTClient 端获取每⼀个⼀个 Pod 利⽤ 率或原始值的平均值，然后和 HPA 中定义的指标进⾏对⽐，同时计算出需要伸缩的具体值并进⾏相应 的操作。⽬前， HPA 可以从两个地⽅获取数据：

- Heapster     
- ⾃定义监控

同样的，我们来创建⼀个 Deployment 管理的 Nginx Pod，然后利⽤ HPA 来进⾏⾃动扩缩容。定义 Deployment 的 YAML ⽂件如下：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx-demo
  name: hpa-nginx-deploy
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 15
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```

然后创建 Deployment 

```
kubectl create -f hpa-deploy-gomo.yaml
```

创建⼀个 HPA ，可以使⽤ kubectl autoscale 命令来创建：此命令创建了⼀个关联资源 hpa-nginx-deploy 的 HPA ，最⼩的 pod 副本数为1，最⼤为10。 HPA 会 根据设定的 cpu使⽤率（10%）动态的增加或者减少pod数量。

```
[root@adm-master1 home]# kubectl autoscale deployment hpa-nginx-deploy --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-nginx-deploy autoscaled
[root@adm-master1 home]# kubectl get hpa
NAME               REFERENCE                     TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
hpa-nginx-deploy   Deployment/hpa-nginx-deploy   <unknown>/10%   1         10        0          8s
[root@adm-master1 home]# kubectl get deployment hpa-nginx-deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
hpa-nginx-deploy   1/1     1            1           5m39s
[root@adm-master1 home]# kubectl get hpa
NAME               REFERENCE                     TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
hpa-nginx-deploy   Deployment/hpa-nginx-deploy   <unknown>/10%   1         10        1          5m38
```

PS：unknown的原因是因为 1.17的k8s 不支持heapster了 ！

安装heapster之后就可以了，自行百度.....

#### 9.8.1 基于metrics server的HPA

**metrics-server**

从Kubernetes 1.8开始，Kubernetes通过Metrics API提供资源使用指标，例如容器CPU和内存使用。这些度量可以由用户直接访问，例如通过使用`kubectl top`命令，或者由群集中的控制器（例如Horizontal Pod Autoscaler）使用来进行决策。



部署文件：

auth-delegator.yaml metrics-apiservice.yaml     metrics-server-service.yaml

auth-reader.yaml   metrics-server-deployment.yaml resource-reader.yaml



```bash
for i in auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml ;do wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/metrics-server/$i;done
```

直接下载下来的文件需要进行修改使用



auth-delegator.yaml

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:auth-delegator
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
```



auth-reader.yaml

```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-auth-reader
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
```



metrics-apiservice.yaml

```
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  service:
    name: metrics-server
    namespace: kube-system
  group: metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
```



metrics-server-deployment.yaml

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: metrics-server-config
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: EnsureExists
data:
  NannyConfiguration: |-
    apiVersion: nannyconfig/v1alpha1
    kind: NannyConfiguration
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server-v0.3.1
  namespace: kube-system
  labels:
    k8s-app: metrics-server
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    version: v0.3.1
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
      version: v0.3.1
  template:
    metadata:
      name: metrics-server
      labels:
        k8s-app: metrics-server
        version: v0.3.1
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: metrics-server
      containers:
      - name: metrics-server
        image: k8s.gcr.io/metrics-server-amd64:v0.3.1
        command:
        - /metrics-server
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP
        ports:
        - containerPort: 443
          name: https
          protocol: TCP
      - name: metrics-server-nanny
        image: k8s.gcr.io/addon-resizer:1.8.4
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 5m
            memory: 50Mi
        env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        volumeMounts:
        - name: metrics-server-config-volume
          mountPath: /etc/config
        command:
          - /pod_nanny
          - --config-dir=/etc/config
          #- --cpu={{ base_metrics_server_cpu }}
          - --cpu=20m
          - --extra-cpu=0.5m
          #- --memory={{ base_metrics_server_memory }}
          #- --extra-memory={{ metrics_server_memory_per_node }}Mi
          - --memory=50Mi
          - --extra-memory=5Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          # Specifies the smallest cluster (defined in number of nodes)
          # resources will be scaled to.
          #- --minClusterSize={{ metrics_server_min_cluster_size }}
          - --minClusterSize=1
      volumes:
        - name: metrics-server-config-volume
          configMap:
            name: metrics-server-config
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
```

![image.png](https://s1.51cto.com/images/20190324/1553401225108914.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

10250是https端口，连接它时需要提供证书，所以加上--kubelet-insecure-tls，表示不验证客户端证书，此前的版本中使用--source=这个参数来指定不验证客户端证书。

metrics-server这个容器不能通过CoreDNS 10.96.0.10:53 解析各Node的主机名，metrics-server连节点时默认是连接节点的主机名，需要加个参数，让它连接节点的IP：“--kubelet-preferred-address-types=InternalIP”



metrics-server-service.yaml



```
apiVersion: v1
kind: Service
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "Metrics-server"
spec:
  selector:
    k8s-app: metrics-server
  ports:
  - port: 443
    protocol: TCP
    targetPort: https
```



resource-reader.yaml

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - nodes/stats
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - "extensions"
  resources:
  - deployments
  verbs:
  - get
  - list
  - update
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
```

![image.png](https://s1.51cto.com/images/20190324/1553401285455813.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

部署完成后执行

kubectl get pod -n kube-system查看是否部署成功

![image.png](https://s1.51cto.com/images/20190323/1553355630865648.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

kubectl top pod/node

![image.png](https://s1.51cto.com/images/20190323/1553355708696710.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

如果出现如下报错，请尝试在pod所在节点重启kubelet

Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)



**HPA**

Horizontal Pod Autoscaler根据观察到的CPU利用率自动调整复制控制器，部署或副本集中的pod数量（或者，使用[自定义度量标准](https://git.k8s.io/community/contributors/design-proposals/instrumentation/custom-metrics-api.md)支持，根据其他一些应用程序提供的度量标准）。请注意，Horizontal Pod Autoscaling不适用于无法缩放的对象，例如DaemonSet。

如果某些pod的容器没有设置相关的资源请求，则不会定义pod的CPU利用率，并且autoscaler不会对该度量标准采取任何操作。

![image.png](https://s1.51cto.com/images/20190324/1553400334446179.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

算法：desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]





cat nginx-deploy-hpa.yaml 

```bash
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deploy-hpa
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx-hpa
    spec:
      containers:
      - name: nginx-hpa
        image: nginx:1.8
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 30m
            memory: 30Mi
          limits:
            cpu: 30m
            memory: 30Mi
```

创建HPA

命令创建

```bash
kubectl autoscale deploy nginx-deploy-hpa --min=2 --max=10 --cpu-precent=30
```

文件创建

vi nginx-hpa.yaml

```bash
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-deploy-hpa
  namespace: default
spec:
  maxReplicas: 10
  minReplicas: 2
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: nginx-deploy-hpa
  targetCPUUtilizationPercentage: 30
```

![image.png](https://s1.51cto.com/images/20190324/1553399502634899.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

对pod进行压测，可以看出pod数量自动增长

![image.png](https://s1.51cto.com/images/20190324/1553399741133257.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

![image.png](https://s1.51cto.com/images/20190324/1553399785183345.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

结束压测，过一段时间，pod数量会自动减少

![image.png](https://s1.51cto.com/images/20190324/1553400232974937.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

**（kube 1.16+ 之后安装metrics-server）**

报错未解决：

```
[root@adm-master1 home]# kubectl top nodes
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)
[root@adm-master1 home]# kubectl top pods
Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)
```

 

 

## 10、服务质量

在kubernetes中，每个POD都有个QoS标记，通过这个Qos标记来对POD进行服务质量管理。QoS的英文全称为"Quality of Service",中文名为"服务质量"，它取决于用户对服务质量的预期，也就是期望的服务质量。对于POD来说，服务质量体现在两个指标上，一个指标是CPU，另一个指标是内存。在实际运行过程中，当NODE节点上内存资源紧张的时候，kubernetes根据POD具有的不同QoS标记，采取不同的处理策略。

### 10.1	QoS分类与举例

Qos的分类有3种：

![image-20200102142159490](C:\Users\wangjiadesx\AppData\Roaming\Typora\typora-user-images\image-20200102142159490.png)

| BestEffort     | POD中的所有容器都没有指定CPU和内存的requests和limits，那么这个POD的QoS就是BestEffort级别 |
| -------------- | ------------------------------------------------------------ |
| **Burstable**  | POD中只要有一个容器，这个容器requests和limits的设置同其他容器设置的不一致，那么这个POD的QoS就是Burstable级别 |
| **Guaranteed** | POD中所有容器都必须统一设置了limits，并且设置参数都一致，如果有一个容器要设置requests，那么所有容器都要设置，并设置参数同limits一致，那么这个POD的QoS就是Guaranteed级别 |

举例：

- BestEffort

```yaml
containers:
    name: foo
        resources:
    name: bar
        resources:
```

- Burstable

```
containers:
    name: gomo
        resources:
            limits:
                cpu: 10m
                memory: 1Gi
            requests:
                cpu: 10m
                memory: 1Gi
    name: 3g

---
containers:
    name: gomo
        resources:
            limits:
                memory: 1Gi
    name: 3g
        resources:
            limits:
                cpu: 100m
---
containers:
    name: gomo
        resources:
            requests:
                cpu: 10m
                memory: 1Gi
    name: 3g
```

- Guaranteed

```
---
containers:
    name: foo
        resources:
            limits:
                cpu: 10m
                memory: 1Gi
    name: bar
        resources:
            limits:
                cpu: 100m
                memory: 100Mi
---
containers:
    name: foo
        resources:
            limits:
                cpu: 10m
                memory: 1Gi
            requests:
                cpu: 10m
                memory: 1Gi
---                
    name: bar
        resources:
            limits:
                cpu: 100m
                memory: 100Mi
            requests:
                cpu: 10m
                memory: 1Gi
```



### 10.2可压缩资源与不可压缩资源

Kubernetes是一个容器集群管理平台，Kubernetes需要统计整体平台的资源使用情况，合理地将资源分配给容器使用，并且要保证容器生命周期内有足够的资源来保证其运行。 同时，如果资源发放是独占的，即资源已发放给了个容器，同样的资源不会发放给另外一个容器，对于空闲的容器来说占用着没有使用的资源比如CPU是非常浪费的，Kubernetes需要考虑如何在优先度和公平性的前提下提高资源的利用率。为了实现资源被有效调度和分配同时提高资源的利用率，Kubernetes采用request和limit两种限制类型来对资源进行分配。

1 kuberneters中request和limit限制方式说明

**request**  容器使用的最小资源需求，创建容器的时候，是最小的资源要求只有当节点上可分配资源量>=容器资源请求数时才允许将容器调度到该节点

也就是说，创建容器时候，分配资源是按照`request`指定的值进行独占的，容器至少要保留request指定的资源。

> request参数不限制容器的最大可使用资源。

**limit**  表明容器能使用资源的最大值，设置为0表示使用资源无上限。

2 request和limit的区别

`request` 能够保证Pod有足够的资源来运行，而`limit` 则是防止某个Pod无限制地使用资源，导致其他Pod崩溃。

两者之间必须满足关系:

```bash
0<=request<=limit<=Infinity 
```

> 如果limit为0表示不对资源进行限制，这时可以小于request

Kubernetes中资源通过`request`和`limit`的设置，能够实现容器对资源的更高效的使用。在如果多个容器同时对资源进行充分利用，资源使用尽量的接近limit。 Node节点上的资源总量要小于所有Pod中limit的总和，就会发生资源抢占。

对于资源抢占的情况，Kubernetes根据资源能不能进行伸缩进行分类，分为可压缩资源和不可以压缩资源。

- CPU资源--是现在支持的一种可压缩资源。
- 内存资源和磁盘资源为现在支持的不可压缩资源。

#### 可压缩资源的抢占策略---按照Requst的比值进行分配

假设有pod1~4 分别占用（CPU Requst,CPU limit,Memory Requst, Memory limit）= (1U, 2U, 1G,1G)。当四个pod的负载都很高，CPU使用都超过1U的情况下，这个时候每个pod将按照request设置的CPU比例进行时间片调度。由于4个Pod设置的request都为1U，发生资源抢占时，每个Pod分到的CPU时间片为1U/(1U)*4，实际占用的CPU核数为1U。

> 这里涉及到docker关于CPU占用的策略。CPU占用可以按照占用指定核或者占用时间片资源来区分。默认情况下使用的是时间片。

#### 不可压缩资源的抢占策略---按照优先级的不同，进行Pod的驱逐

对于不可压缩资源，如果发生资源抢占，则会按照优先级的高低进行Pod的驱逐。驱逐的策略为： 优先驱逐request=limit=0的Pod，其次驱逐0<request<limit<Infinity(limit为0的情况也包括在内)。 0<request==limit的Pod的会被保留，除非出现删除其他Pod后，节点上剩余资源仍然没有达到Kubernetes需要的剩余资源的需求。

由于对于不可压缩资源，发生抢占的情况会出Pod被意外Kill掉的情况，所以建议对于不可以压缩资源(Memory，Disk)的设置成0<request==limit。

### 10.3Pod被Kill场景与顺序

- QoS级别决定了kubernetes处理这些POD的方式，我们以内存资源为例：

  - 当NODE节点上内存资源不够的时候，QoS级别是BestEffort的POD会最先被kill掉；当NODE节点上内存资源充足的时候，QoS级别是BestEffort的POD可以使用NODE节点上剩余的所有内存资源。
  - 当NODE节点上内存资源不够的时候，如果QoS级别是BestEffort的POD已经都被kill掉了，那么会查找QoS级别是Burstable的POD，并且这些POD使用的内存已经超出了requests设置的内存值，这些被找到的POD会被kill掉；当NODE节点上内存资源充足的时候，QoS级别是Burstable的POD会按照requests和limits的设置来使用。
  - 当NODE节点上内存资源不够的时候，如果QoS级别是BestEffort和Burstable的POD都已经被kill掉了，那么会查找QoS级别是Guaranteed的POD，并且这些POD使用的内存已经超出了limits设置的内存值，这些被找到的POD会被kill掉；当NODE节点上内存资源充足的时候，QoS级别是Burstable的POD会按照requests和limits的设置来使用。
  - 从容器的角度出发，为了限制容器使用的CPU和内存，是通过cgroup来实现的，目前kubernetes的QoS只能管理CPU和内存，所以kubernetes现在也是通过对cgroup的配置来实现QoS管理的。

### 10.4QoS使用最佳实践

资源对象 **limit-range.yaml** 内容如下：

```
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-test
spec:
  limits:
    - type: Pod        #对Pod中所有容器资源总和进行限制
      max:
        cpu: 4000m
        memory: 2048Mi 
      min:
        cpu: 10m
        memory: 128Mi 
      maxLimitRequestRatio:
        cpu: 5
        memory: 5
    - type: Container  #对Pod中所有容器资源进行限制
      max:
        cpu: 2000m
        memory: 1024Mi
      min:
        cpu: 10m
        memory: 128Mi 
      maxLimitRequestRatio:
        cpu: 5
        memory: 5
      default:
        cpu: 1000m
        memory: 512Mi
      defaultRequest:
        cpu: 500m
        memory: 256Mi
```



```
[root@adm-master1 home]# kubectl apply -f test.yaml -n gomo-test
limitrange/limit-test created
[root@adm-master1 home]# kubectl describe limitrange limit-test -n gomo-test
Name:       limit-test
Namespace:  gomo-test
Type        Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---    ---  ---------------  -------------  -----------------------
Pod         cpu       10m    4    -                -              5
Pod         memory    128Mi  2Gi  -                -              5
Container   memory    128Mi  1Gi  256Mi            512Mi          5
Container   cpu       10m    2    500m             1              5
```

创建 Pod 来进行测试

```
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: nginx1
    image: nginx:latest
  - name: nginx2
    image: nginx:latest
    resources:
      limits:
        cpu: "3000m"
        memory: "512Mi"
```

创建查看报错：执行 Kubectl 命令创建 Pod 时并没有通过验证，并且已经提示 CPU 不能超过 2 个：

```
[root@adm-master1 home]# kubectl create -f qos.yaml -n gomo-test
Error from server (Forbidden): error when creating "qos.yaml": pods "test" is forbidden: maximum cpu usage per Container is 2, but limit is 3
```





上面设置 Min 中 CPU 和 Memory 的值分别为 10m 与 128Mi，下面创建一个 Pod 并设置其中某一个容器 Requests 值小于 LimitRange 中设置的值，是否能起到限制作用。Pod 内容如下：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: nginx1
    image: nginx:latest
  - name: nginx2
    image: nginx:latest
    resources:
      requests:
        cpu: "100m"
        memory: "64Mi"
```

执行 Kubectl 命令创建 Pod 时并没有通过验证，并且已经提示 Memory 不能低于 128Mi 大小：

```
[root@adm-master1 home]# kubectl create -f qos.yaml -n gomo-test
Error from server (Forbidden): error when creating "qos.yaml": pods "test" is forbidden: [minimum memory usage per Container is 128Mi, but request is 64Mi, cpu max limit to request ratio per Container is 5, but provided ratio is 10.000000, memory max limit to request ratio per Container is 5, but provided ratio is 8.000000]
```

上面 LimitRange 中设置 maxLimitRequestRatio 值为 5，就是限制 Pod 中容器 CPU 和 Memory 的 limit/request 的值小于 5,这里测试一下设置内存 limit/request 中值超过 5 创建 Pod 是否会报错。Pod 内容如下：

```
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: nginx1
    image: nginx:latest
  - name: nginx2
    image: nginx:latest
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "1024Mi"
```

```
[root@adm-master1 home]# kubectl create -f qos.yaml -n gomo-test
Error from server (Forbidden): error when creating "qos.yaml": pods "test" is forbidden: memory max limit to request ratio per Container is 5, but provided ratio is 8.000000
```

